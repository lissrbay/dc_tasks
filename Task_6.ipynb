{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
    "- `news_train.txt` тренировочное множество\n",
    "- `news_test.txt` тренировочное множество\n",
    "\n",
    "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
    "\n",
    "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
    "\n",
    ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача\n",
    "\n",
    "1. Обработать данные, получив для каждого текста набор токенов\n",
    "Обработать токены с помощью (один вариант из трех):\n",
    "\n",
    "    - pymorphy2\n",
    "\n",
    "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
    "    \n",
    "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
    "    \n",
    "    \n",
    "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
    "\n",
    "3. Реализовать алгоритм классификации, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
    "     - SVM\n",
    "     - наивный байесовский классификатор\n",
    "     - логистическая регрессия\n",
    "    \n",
    "\n",
    "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install pymorphy2\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lissrbay/anaconda3/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "[nltk_data] Downloading package punkt to /home/lissrbay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import pymorphy2\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import FloatTensor, LongTensor\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tqdm.pandas()\n",
    "nltk.download('punkt')\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "df = pd.read_csv('news_train.txt', sep='\\t',header=None )\n",
    "df.columns = ['category', 'title', 'text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### №1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:22<00:00, 677.78it/s]\n",
      "100%|██████████| 15000/15000 [00:00<00:00, 33969.05it/s]\n",
      "100%|██████████| 15000/15000 [09:34<00:00, 26.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sport</td>\n",
       "      <td>Овечкин пожертвовал детской хоккейной школе ав...</td>\n",
       "      <td>[нападать, вашингтон, кэпиталзти, александр, о...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>culture</td>\n",
       "      <td>Рекордно дорогую статую майя признали подделкой</td>\n",
       "      <td>[власть, мексика, объявить, подделка, статуя, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>science</td>\n",
       "      <td>Samsung представила флагман в защищенном корпусе</td>\n",
       "      <td>[южнокорейский, samsung, анонсировать, защитит...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>С футболиста «Спартака» сняли четырехматчевую ...</td>\n",
       "      <td>[комитет, кдк, рфс, снять, дисквалификация, с,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>media</td>\n",
       "      <td>Hopes &amp; Fears объединится с The Village</td>\n",
       "      <td>[hopes, fears, объявить, о, свой, слияние, с, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                              title  \\\n",
       "0    sport  Овечкин пожертвовал детской хоккейной школе ав...   \n",
       "1  culture    Рекордно дорогую статую майя признали подделкой   \n",
       "2  science   Samsung представила флагман в защищенном корпусе   \n",
       "3    sport  С футболиста «Спартака» сняли четырехматчевую ...   \n",
       "4    media            Hopes & Fears объединится с The Village   \n",
       "\n",
       "                                                text  \n",
       "0  [нападать, вашингтон, кэпиталзти, александр, о...  \n",
       "1  [власть, мексика, объявить, подделка, статуя, ...  \n",
       "2  [южнокорейский, samsung, анонсировать, защитит...  \n",
       "3  [комитет, кдк, рфс, снять, дисквалификация, с,...  \n",
       "4  [hopes, fears, объявить, о, свой, слияние, с, ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].progress_apply(word_tokenize)\n",
    "df['text'] = df['text'].progress_apply(lambda x: [word for word in x if word.isalpha()])\n",
    "df['text'] = df['text'].progress_apply(lambda x: [morph.parse(word)[0].normal_form for word in x])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### №2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec()\n",
    "sentences = df['text'].values\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64809694, 79175640)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример семантической связи: близкие слова для слова вратарь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('голкипер', 0.8509472012519836),\n",
       " ('форвард', 0.8013597726821899),\n",
       " ('полузащитник', 0.7753452062606812),\n",
       " ('хоккеист', 0.7597266435623169),\n",
       " ('нападать', 0.7316120862960815),\n",
       " ('денисов', 0.7236849069595337),\n",
       " ('малкин', 0.7207446694374084),\n",
       " ('кокорин', 0.7188119292259216),\n",
       " ('семин', 0.7075212001800537),\n",
       " ('пингвинс', 0.7069272994995117)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"вратарь\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример семантической связи: близкие слова для слова вклад и те, которые по смыслу далеки от темы банка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('депозит', 0.6161470413208008),\n",
       " ('кредит', 0.5344643592834473),\n",
       " ('страхование', 0.5078973770141602),\n",
       " ('взнос', 0.49820154905319214),\n",
       " ('ссуда', 0.4761122465133667),\n",
       " ('ставка', 0.4754984974861145),\n",
       " ('банковский', 0.47037452459335327),\n",
       " ('страхов', 0.4685913622379303),\n",
       " ('кредитование', 0.46608319878578186),\n",
       " ('актив', 0.46394848823547363)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['вклад'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('выдающийся', 0.4104170501232147),\n",
       " ('дебют', 0.4048970937728882),\n",
       " ('мастерство', 0.3994008004665375),\n",
       " ('уайлёд', 0.3963795006275177),\n",
       " ('режиссура', 0.37563467025756836),\n",
       " ('отчисление', 0.3611883521080017),\n",
       " ('подвиг', 0.35683977603912354),\n",
       " ('задекларировать', 0.35437875986099243),\n",
       " ('валери', 0.35391169786453247),\n",
       " ('веха', 0.352017879486084)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['вклад'], negative=[\"банк\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### №3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df['text'].values\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "vectorizer.fit_transform([x for x in X_train])\n",
    "idf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15000 [00:00<?, ?it/s]/home/lissrbay/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "100%|██████████| 15000/15000 [00:24<00:00, 624.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>wv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sport</td>\n",
       "      <td>Овечкин пожертвовал детской хоккейной школе ав...</td>\n",
       "      <td>[нападать, вашингтон, кэпиталзти, александр, о...</td>\n",
       "      <td>[[0.7729980481114793, -3.510473666990057, -0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>culture</td>\n",
       "      <td>Рекордно дорогую статую майя признали подделкой</td>\n",
       "      <td>[власть, мексика, объявить, подделка, статуя, ...</td>\n",
       "      <td>[[-0.5589148749907812, -0.127827702910595, 1.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>science</td>\n",
       "      <td>Samsung представила флагман в защищенном корпусе</td>\n",
       "      <td>[южнокорейский, samsung, анонсировать, защитит...</td>\n",
       "      <td>[[-0.0800700051819577, -0.4852414146230063, 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>С футболиста «Спартака» сняли четырехматчевую ...</td>\n",
       "      <td>[комитет, кдк, рфс, снять, дисквалификация, с,...</td>\n",
       "      <td>[[-1.7253159720506241, -3.6982621303412007, 2....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>media</td>\n",
       "      <td>Hopes &amp; Fears объединится с The Village</td>\n",
       "      <td>[hopes, fears, объявить, о, свой, слияние, с, ...</td>\n",
       "      <td>[[1.7165771966827088, 0.11026712150677391, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                              title  \\\n",
       "0    sport  Овечкин пожертвовал детской хоккейной школе ав...   \n",
       "1  culture    Рекордно дорогую статую майя признали подделкой   \n",
       "2  science   Samsung представила флагман в защищенном корпусе   \n",
       "3    sport  С футболиста «Спартака» сняли четырехматчевую ...   \n",
       "4    media            Hopes & Fears объединится с The Village   \n",
       "\n",
       "                                                text  \\\n",
       "0  [нападать, вашингтон, кэпиталзти, александр, о...   \n",
       "1  [власть, мексика, объявить, подделка, статуя, ...   \n",
       "2  [южнокорейский, samsung, анонсировать, защитит...   \n",
       "3  [комитет, кдк, рфс, снять, дисквалификация, с,...   \n",
       "4  [hopes, fears, объявить, о, свой, слияние, с, ...   \n",
       "\n",
       "                                                  wv  \n",
       "0  [[0.7729980481114793, -3.510473666990057, -0.6...  \n",
       "1  [[-0.5589148749907812, -0.127827702910595, 1.7...  \n",
       "2  [[-0.0800700051819577, -0.4852414146230063, 1....  \n",
       "3  [[-1.7253159720506241, -3.6982621303412007, 2....  \n",
       "4  [[1.7165771966827088, 0.11026712150677391, -0....  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embedding(sentence, embedding_size=100):\n",
    "    vec = np.zeros(embedding_size).reshape((1, embedding_size))\n",
    "    count = 0.\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            vec += w2v_model[word].reshape((1, embedding_size)) * idf[word]\n",
    "            count += 1.\n",
    "        except KeyError: \n",
    "            continue\n",
    "        vec /= (count if count > 0 else 1)\n",
    "    return vec\n",
    "\n",
    "df['wv'] = df['text'].progress_apply(lambda x: embedding(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    X, y = df['wv'].values, df['category']\n",
    "    X_ = []\n",
    "    for i in X:\n",
    "        X_.append(np.array(i).reshape(100))\n",
    "    X = np.array(X_)\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return X, y\n",
    "X, y = prepare_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  27 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=7)]: Done  45 out of  45 | elapsed:  4.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'kernel': 'rbf'}\n",
      "Accuracy 0.851\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n",
    "classifier_svm = SVC(class_weight='balanced', gamma='auto')\n",
    "params = {\n",
    "    'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    'C': [1, 1e-1, 1e-2]\n",
    "}\n",
    "gs = GridSearchCV(classifier_svm, params, cv=5, n_jobs=7, verbose=2)\n",
    "gs.fit(X_train, y_train)\n",
    "print('Best parameters:', gs.best_params_)\n",
    "y_pred = gs.predict(X_test)\n",
    "print('Accuracy', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  27 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=7)]: Done  60 out of  60 | elapsed:  5.3min finished\n",
      "/home/lissrbay/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/lissrbay/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 0.1, 'l1_ratio': 0.001}\n",
      "Accuracy 0.8316666666666667\n"
     ]
    }
   ],
   "source": [
    "classifier_lr = LogisticRegression(class_weight='balanced', penalty='elasticnet', solver='saga')\n",
    "params = {\n",
    "    'l1_ratio': [1, 1e-1, 1e-2, 1e-3],\n",
    "    'C': [1, 1e-1, 1e-2]\n",
    "}\n",
    "gs = GridSearchCV(classifier_lr, params, cv=5, n_jobs=7, verbose=2)\n",
    "gs.fit(X_train, y_train)\n",
    "print('Best parameters:', gs.best_params_)\n",
    "\n",
    "y_pred = gs.predict(X_test)\n",
    "print('Accuracy', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
